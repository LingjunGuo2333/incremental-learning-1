{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxSJ8QgK5XXurGuf8F9Pp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jianhaoma/incremental-learning/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t013rrKBB80H"
      },
      "outputs": [],
      "source": [
        "\"\"\"CIFAR-10-Resnet.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/147UWhdtpPdEzlqaIc_iIQN06dYTTPsvV\n",
        "\"\"\"\n",
        "# Imports\n",
        "import wandb\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "# parsers\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training VGG')\n",
        "parser.add_argument('--lr', default=1e-1, type=float, help='learning rate')\n",
        "parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
        "parser.add_argument('--batch_size', default=512, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=310, type=int, help='number of epoch')\n",
        "parser.add_argument('--model', default='resnet18', help='type of model')\n",
        "parser.add_argument('--data', default='Cifar10', help='type of dataset')\n",
        "parser.add_argument('--decay_rate', default=0.33, type=float, help='the decay rate in each stepsize decay')\n",
        "parser.add_argument('--decay_stepsize', default=50, type=int, help='the decay time')\n",
        "parser.add_argument('--num_run', default=1, type=int, help='the number of run')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "# fix random seed\n",
        "clear_output()\n",
        "seed = args.seed\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# plotting style\n",
        "plt.style.use('seaborn-paper')\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# Define Data Loaders\n",
        "\n",
        "from typing import List\n",
        "\n",
        "_CONV_OPTIONS = {\"kernel_size\": 3, \"padding\": 1, \"stride\": 1}\n",
        "\n",
        "def get_activation(activation: str):\n",
        "    if activation == 'relu':\n",
        "        return torch.nn.ReLU()\n",
        "    elif activation == 'hardtanh':\n",
        "        return torch.nn.Hardtanh()\n",
        "    elif activation == 'leaky_relu':\n",
        "        return torch.nn.LeakyReLU()\n",
        "    elif activation == 'selu':\n",
        "        return torch.nn.SELU()\n",
        "    elif activation == 'elu':\n",
        "        return torch.nn.ELU()\n",
        "    elif activation == \"tanh\":\n",
        "        return torch.nn.Tanh()\n",
        "    elif activation == \"softplus\":\n",
        "        return torch.nn.Softplus()\n",
        "    elif activation == \"sigmoid\":\n",
        "        return torch.nn.Sigmoid()\n",
        "    else:\n",
        "        raise NotImplementedError(\"unknown activation function: {}\".format(activation))\n",
        "\n",
        "def get_pooling(pooling: str):\n",
        "    if pooling == 'max':\n",
        "        return torch.nn.MaxPool2d((2, 2))\n",
        "    elif pooling == 'average':\n",
        "        return torch.nn.AvgPool2d((2, 2))\n",
        "\n",
        "\n",
        "def fully_connected_net(dataset_name: str, widths: List[int], activation: str, bias: bool = True) -> nn.Module:\n",
        "    modules = [nn.Flatten()]\n",
        "    for l in range(len(widths)):\n",
        "        prev_width = widths[l - 1] if l > 0 else 3*32**2\n",
        "        modules.extend([\n",
        "            nn.Linear(prev_width, widths[l], bias=bias),\n",
        "            get_activation(activation),\n",
        "        ])\n",
        "    modules.append(nn.Linear(widths[-1], 10, bias=bias))\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "def fully_connected_net_bn(dataset_name: str, widths: List[int], activation: str, bias: bool = True) -> nn.Module:\n",
        "    modules = [nn.Flatten()]\n",
        "    for l in range(len(widths)):\n",
        "        prev_width = widths[l - 1] if l > 0 else 3*32**2\n",
        "        modules.extend([\n",
        "            nn.Linear(prev_width, widths[l], bias=bias),\n",
        "            get_activation(activation),\n",
        "            nn.BatchNorm1d(widths[l])\n",
        "        ])\n",
        "    modules.append(nn.Linear(widths[-1], 10, bias=bias))\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "def convnet(dataset_name: str, widths: List[int], activation: str, pooling: str, bias: bool) -> nn.Module:\n",
        "    modules = []\n",
        "    size = 32\n",
        "    for l in range(len(widths)):\n",
        "        prev_width = widths[l - 1] if l > 0 else 3\n",
        "        modules.extend([\n",
        "            nn.Conv2d(prev_width, widths[l], bias=bias, **_CONV_OPTIONS),\n",
        "            get_activation(activation),\n",
        "            get_pooling(pooling),\n",
        "        ])\n",
        "        size //= 2\n",
        "    modules.append(nn.Flatten())\n",
        "    modules.append(nn.Linear(widths[-1]*size*size, 10))\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "def convnet_bn(dataset_name: str, widths: List[int], activation: str, pooling: str, bias: bool) -> nn.Module:\n",
        "    modules = []\n",
        "    size = 32\n",
        "    for l in range(len(widths)):\n",
        "        prev_width = widths[l - 1] if l > 0 else 3\n",
        "        modules.extend([\n",
        "            nn.Conv2d(prev_width, widths[l], bias=bias, **_CONV_OPTIONS),\n",
        "            get_activation(activation),\n",
        "            nn.BatchNorm2d(widths[l]),\n",
        "            get_pooling(pooling),\n",
        "        ])\n",
        "        size //= 2\n",
        "    modules.append(nn.Flatten())\n",
        "    modules.append(nn.Linear(widths[-1]*size*size, 10))\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "def make_deeplinear(L: int, d: int, seed=8):\n",
        "    torch.manual_seed(seed)\n",
        "    layers = []\n",
        "    for l in range(L):\n",
        "        layer = nn.Linear(d, d, bias=False)\n",
        "        nn.init.xavier_normal_(layer.weight)\n",
        "        layers.append(layer)\n",
        "    network = nn.Sequential(*layers)\n",
        "    return network.cuda()\n",
        "\n",
        "def make_one_layer_network(h=10, seed=0, activation='tanh', sigma_w=1.9):\n",
        "    torch.manual_seed(seed)\n",
        "    network = nn.Sequential(\n",
        "        nn.Linear(1, h, bias=True),\n",
        "        get_activation(activation),\n",
        "        nn.Linear(h, 1, bias=False),\n",
        "    )\n",
        "    nn.init.xavier_normal_(network[0].weight, gain=sigma_w)\n",
        "    nn.init.zeros_(network[0].bias)\n",
        "    nn.init.xavier_normal_(network[2].weight)\n",
        "    return network\n",
        "\n",
        "\n",
        "def load_architecture(arch_id: str, dataset_name: str) -> nn.Module:\n",
        "    #  ======   fully-connected networks =======\n",
        "    if arch_id == 'fc-relu':\n",
        "        return fully_connected_net(dataset_name, [200, 200], 'relu', bias=True)\n",
        "    elif arch_id == 'fc-elu':\n",
        "        return fully_connected_net(dataset_name, [200, 200], 'elu', bias=True)\n",
        "    elif arch_id == 'fc-tanh':\n",
        "        return fully_connected_net(dataset_name, [200, 200], 'tanh', bias=True)\n",
        "    elif arch_id == 'fc-hardtanh':\n",
        "        return fully_connected_net(dataset_name, [200, 200], 'hardtanh', bias=True)\n",
        "    elif arch_id == 'fc-softplus':\n",
        "        return fully_connected_net(dataset_name, [200, 200], 'softplus', bias=True)\n",
        "\n",
        "    #  ======   convolutional networks =======\n",
        "    elif arch_id == 'cnn-relu':\n",
        "        return convnet(dataset_name, [32, 32], activation='relu', pooling='max', bias=True)\n",
        "    elif arch_id == 'cnn-elu':\n",
        "        return convnet(dataset_name, [32, 32], activation='elu', pooling='max', bias=True)\n",
        "    elif arch_id == 'cnn-tanh':\n",
        "        return convnet(dataset_name, [32, 32], activation='tanh', pooling='max', bias=True)\n",
        "    elif arch_id == 'cnn-avgpool-relu':\n",
        "        return convnet(dataset_name, [32, 32], activation='relu', pooling='average', bias=True)\n",
        "    elif arch_id == 'cnn-avgpool-elu':\n",
        "        return convnet(dataset_name, [32, 32], activation='elu', pooling='average', bias=True)\n",
        "    elif arch_id == 'cnn-avgpool-tanh':\n",
        "        return convnet(dataset_name, [32, 32], activation='tanh', pooling='average', bias=True)\n",
        "\n",
        "    #  ======   convolutional networks with BN =======\n",
        "    elif arch_id == 'cnn-bn-relu':\n",
        "        return convnet_bn(dataset_name, [32, 32], activation='relu', pooling='max', bias=True)\n",
        "    elif arch_id == 'cnn-bn-elu':\n",
        "        return convnet_bn(dataset_name, [32, 32], activation='elu', pooling='max', bias=True)\n",
        "    elif arch_id == 'cnn-bn-tanh':\n",
        "        return convnet_bn(dataset_name, [32, 32], activation='tanh', pooling='max', bias=True)\n",
        "\n",
        "\n",
        "    # ====== additional networks ========\n",
        "    # elif arch_id == 'transformer':\n",
        "        # return TransformerModelFixed()\n",
        "    elif arch_id == 'deeplinear':\n",
        "        return make_deeplinear(20, 50)\n",
        "    elif arch_id == 'regression':\n",
        "        return make_one_layer_network(h=100, activation='tanh')\n",
        "\n",
        "    # ======= vary depth =======\n",
        "    elif arch_id == 'fc-tanh-depth1':\n",
        "        return fully_connected_net(dataset_name, [200], 'tanh', bias=True)\n",
        "    elif arch_id == 'fc-tanh-depth2':\n",
        "        return fully_connected_net(dataset_name, [200, 200], 'tanh', bias=True)\n",
        "    elif arch_id == 'fc-tanh-depth3':\n",
        "        return fully_connected_net(dataset_name, [200, 200, 200], 'tanh', bias=True)\n",
        "    elif arch_id == 'fc-tanh-depth4':\n",
        "        return fully_connected_net(dataset_name, [200, 200, 200, 200], 'tanh', bias=True)\n",
        "\n",
        "# load training data\n",
        "def load_train_data(batch_size, param_mean, param_std, num_workers):\n",
        "    transform_train = transforms.Compose([\n",
        "        torchvision.transforms.RandomCrop(32, padding=4),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(param_mean, param_std),\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                             train=True,\n",
        "                                             download=True,\n",
        "                                             transform=transform_train)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=num_workers)\n",
        "\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "# load test data and validation data (here, test == validation)\n",
        "def load_test_data(batch_size, param_mean, param_std, num_workers):\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(param_mean, param_std),\n",
        "    ])\n",
        "\n",
        "    test_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                            train=False,\n",
        "                                            download=True,\n",
        "                                            transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=num_workers)\n",
        "\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "# Specify device and load data to device.\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def train_log(loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
        "\n",
        "\n",
        "# define network\n",
        "def create_model():\n",
        "    model = torchvision.models.vgg11(pretrained=False, num_classes=10)\n",
        "    model.conv1 = nn.Conv2d(3,\n",
        "                            64,\n",
        "                            kernel_size=(3, 3),\n",
        "                            stride=(1, 1),\n",
        "                            padding=(1, 1),\n",
        "                            bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    # model.fc = nn.Linear(in_features=512, out_features=10, bias=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Define a hook\n",
        "features = {}\n",
        "def get_features(name):\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        features[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "def initial_test(model, val_loader, device):\n",
        "    # validation phase\n",
        "    model.eval()\n",
        "    f_out = torch.empty((0, 10))\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        num_batch = len(val_loader)\n",
        "        for _, data in enumerate(val_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            f_out = torch.vstack((f_out, outputs.detach().cpu()))\n",
        "    return f_out\n",
        "\n",
        "\n",
        "def model_train(model, device, train_loader, val_loader, EPOCH, criterion,\n",
        "                optimizer, scheduler, model_args=0):\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=100)\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    training_accs = []\n",
        "    test_accs = []\n",
        "    epoch_state_dict = {}\n",
        "    best_acc = 0\n",
        "    name = model_args\n",
        "    F_out = torch.zeros((10000, 10, 1))\n",
        "    CheckPoint = {}\n",
        "    for epoch in range(EPOCH):\n",
        "        model.train()\n",
        "        correct, sum_loss, total = 0, 0, 0\n",
        "        for _, data in enumerate(train_loader, 0):\n",
        "            # prepare data\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(F.softmax(outputs, 1),\n",
        "                             F.one_hot(labels, num_classes=10).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            sum_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            example_ct += len(inputs)\n",
        "            batch_ct += 1\n",
        "\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "        training_acc = correct / total\n",
        "        # print('[epoch:%d] Loss: %.05f | Acc: %.2f%%' %\n",
        "        #       (epoch + 1, sum_loss, 100. * training_acc))\n",
        "        # training_accs.append(training_acc)\n",
        "        wandb.log({\"train_accuracy\": training_acc})\n",
        "\n",
        "        # validation phase\n",
        "        if epoch == EPOCH - 1:\n",
        "            feature_name = 'phi'\n",
        "            if name == 'vgg11':\n",
        "                handle = model.classifier[5].register_forward_hook(get_features(feature_name))\n",
        "                FEATS = []\n",
        "            else:\n",
        "                second_to_last = list(model.__dict__['_modules'].keys())[-2]\n",
        "                handle = getattr(model, second_to_last).register_forward_hook(get_features(feature_name))\n",
        "                FEATS = []\n",
        "\n",
        "        model.eval()\n",
        "        f_out = torch.empty((0, 10))\n",
        "        with torch.no_grad():\n",
        "            correct, total = 0, 0\n",
        "            num_batch = len(val_loader)\n",
        "            for _, data in enumerate(val_loader, 0):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                f_out = torch.vstack((f_out, outputs.detach().cpu()))\n",
        "                if epoch == EPOCH - 1:\n",
        "                    FEATS.append(features[feature_name].cpu().numpy())\n",
        "            F_out = torch.cat([F_out, torch.unsqueeze(f_out, 2)], dim=2)\n",
        "            test_acc = correct / total\n",
        "            # test_accs.append(test_acc)\n",
        "            wandb.log({\"test_accuracy\": test_acc})\n",
        "            # print('[epoch:%d] Test Acc: %.2f%%' % (epoch + 1, 100 * test_acc))\n",
        "            # if best_acc < test_acc:\n",
        "            #     best_acc = test_acc\n",
        "        if epoch == EPOCH - 1:\n",
        "            phi = torch.flatten(torch.from_numpy(FEATS[0]), 1)\n",
        "            for i in range(num_batch - 1):\n",
        "                phi = torch.vstack(\n",
        "                    (phi, torch.flatten(torch.from_numpy(FEATS[i + 1]), 1)))\n",
        "            handle.remove()\n",
        "            if name == 'vgg11':\n",
        "                beta = model.state_dict()['classifier.6.weight'].detach().cpu()\n",
        "            else:\n",
        "                beta = list(model.state_dict().items())[-2][1].detach().cpu()\n",
        "        scheduler.step()\n",
        "\n",
        "        # # plot\n",
        "        # plt.plot(training_accs, linewidth=2, label='train acc')\n",
        "        # plt.plot(test_accs, linewidth=2, label='test acc')\n",
        "        # plt.legend(loc='best')\n",
        "        # plt.xlim(0, epoch + 1)\n",
        "        # plt.ylim(0, 1)\n",
        "        # plt.xlabel(\"epoch\")\n",
        "        # plt.ylabel(\"accuracy\")\n",
        "        # plt.legend(prop={'size': 18})\n",
        "        # axes = plt.gca()\n",
        "        # axes.xaxis.label.set_size(18)\n",
        "        # axes.yaxis.label.set_size(18)\n",
        "        # plt.xticks(color='k', fontsize=14)\n",
        "        # plt.yticks(color='k', fontsize=14)\n",
        "        # plt.grid(True)\n",
        "        # plt.tight_layout()\n",
        "        # plt.savefig('resnet_18_v2')\n",
        "        # plt.clf()\n",
        "\n",
        "    return model, best_acc, beta, phi, F_out\n",
        "\n",
        "\n",
        "# TRAIN\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    from datetime import datetime\n",
        "    from pytz import timezone     \n",
        "\n",
        "    Ue = timezone('US/Eastern')\n",
        "    Ue_time = datetime.now(Ue)\n",
        "    time = Ue_time.strftime('%m-%d-%H-%M')\n",
        "    wandb.init(project=args.model+args.data+time,\n",
        "           entity=\"incremental-learning-basis-decomposition\")\n",
        "    # Define hyperparameters\n",
        "    EPOCH = args.num_epoch\n",
        "    batch_size = args.batch_size\n",
        "    num_run = args.num_run\n",
        "    num_workers = 0\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
        "               'ship', 'truck')\n",
        "    LR = args.lr\n",
        "\n",
        "    # load data and model to device\n",
        "    param_mean = (0.4914, 0.4822, 0.4465)\n",
        "    param_std = (0.2471, 0.2435, 0.2616\n",
        "                 )  # param_mean, param_std = get_mean_and_std(dataset_path)\n",
        "    train_loader = load_train_data(batch_size, param_mean, param_std,\n",
        "                                   num_workers)\n",
        "    val_loader = load_test_data(batch_size, param_mean, param_std, num_workers)\n",
        "    device = get_default_device()\n",
        "\n",
        "    if args.model == 'resnet18':\n",
        "        model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
        "        model.conv1 = nn.Conv2d(3,\n",
        "                            64,\n",
        "                            kernel_size=(3, 3),\n",
        "                            stride=(1, 1),\n",
        "                            padding=(1, 1),\n",
        "                            bias=False)\n",
        "        model.maxpool = nn.Identity()\n",
        "    elif args.model == 'vgg11':\n",
        "        model = torchvision.models.vgg11(pretrained=False, num_classes=10)\n",
        "        model.conv1 = nn.Conv2d(3,\n",
        "                                64,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=(1, 1),\n",
        "                                bias=False)\n",
        "        model.maxpool = nn.Identity()\n",
        "    else:\n",
        "        model = load_architecture(args.model, args.data)\n",
        "\n",
        "    # model = create_model().to(device)\n",
        "    model.to(device)\n",
        "    for i in range(num_run):\n",
        "        #reinitialize model parameters\n",
        "\n",
        "        optimizer = torch.optim.SGD(model.parameters(),\n",
        "                                    lr=LR,\n",
        "                                    momentum=0.9,\n",
        "                                    weight_decay=1e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                    step_size=args.decay_stepsize,\n",
        "                                                    gamma=args.decay_rate)\n",
        "        criterion = nn.MSELoss()\n",
        "        # Start Training model, train_dl, vali_dl, EPOCH, criterion, optimizer, scheduler\n",
        "\n",
        "        init_f_out = initial_test(model, val_loader, device)\n",
        "        model, best_acc, beta, Phi, F_out = model_train(model, device,\n",
        "                                                        train_loader, val_loader,\n",
        "                                                        EPOCH, criterion,\n",
        "                                                        optimizer, scheduler, args.model)\n",
        "        F_out[:, :, 0] = init_f_out\n",
        "        # save model\n",
        "        torch.save(model.state_dict(), args.model+time)\n",
        "        torch.save(beta, args.model+'beta'+time)\n",
        "        torch.save(Phi, args.model+'Phi'+time)\n",
        "        torch.save(F_out, args.model+'F_out'+time)\n",
        "        # End Training\n",
        "        print(\"Training finished! The best accuracy is \", best_acc)\n",
        "\n",
        "        U, S, V = torch.svd(Phi)\n",
        "        beta_star = torch.matmul(beta, V)\n",
        "        Coe = {}\n",
        "        _, _, Iter = F_out.shape\n",
        "        for i in range(20):\n",
        "            u = torch.outer(beta_star[:, i], U[:, i])\n",
        "            norm_u = torch.trace(u.t().mm(u)) / 10000\n",
        "            u = u / norm_u\n",
        "            coe = []\n",
        "            for iter in range(Iter):\n",
        "                coe.append((torch.trace(F_out[:, :, iter].mm(u))) / 10000)\n",
        "            Coe[i + 1] = coe\n",
        "        plt.style.use('seaborn-paper')\n",
        "        plt.rcParams['savefig.dpi'] = 300\n",
        "        plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "        for i in range(5):\n",
        "            i_1 = i + 1\n",
        "            plt.plot(range(Iter), [x for x in Coe[i + 1]][0:Iter],\n",
        "                    linewidth=2,\n",
        "                    label=r'$\\beta_{%s}$' % i_1)\n",
        "        plt.locator_params(axis='x', nbins=8)\n",
        "        plt.legend(prop={'size': 10})\n",
        "        axes = plt.gca()\n",
        "        #plt.xlim(0, 1100)\n",
        "        plt.xlabel(\"Iteration $T$\", color='k')\n",
        "        plt.legend(loc='lower right', prop={'size': 18})\n",
        "        plt.title('the {} run'.format(i+1))\n",
        "        axes = plt.gca()\n",
        "        axes.xaxis.label.set_size(18)\n",
        "        axes.yaxis.label.set_size(18)\n",
        "        plt.xticks(color='k', fontsize=14)\n",
        "        plt.yticks(color='k', fontsize=14)\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(args.model+'beta-5'+time)\n",
        "        plt.clf()\n",
        "\n",
        "        for i in range(20):\n",
        "            i_1 = i + 1\n",
        "            if i < 5:\n",
        "                plt.plot(range(Iter), [x for x in Coe[i + 1]][0:Iter],\n",
        "                        linewidth=2,\n",
        "                        label=r'$\\beta_{%s}$' % i_1)\n",
        "            else:\n",
        "                plt.plot(range(Iter), [x for x in Coe[i + 1]][0:Iter], linewidth=2)\n",
        "        plt.locator_params(axis='x', nbins=8)\n",
        "        plt.legend(prop={'size': 10})\n",
        "        axes = plt.gca()\n",
        "        plt.xlabel(\"Iteration $T$\", color='k')\n",
        "        plt.legend(loc='lower right', prop={'size': 18})\n",
        "        plt.title('the {} run'.format(i+1))\n",
        "        axes = plt.gca()\n",
        "        axes.xaxis.label.set_size(18)\n",
        "        axes.yaxis.label.set_size(18)\n",
        "        plt.xticks(color='k', fontsize=14)\n",
        "        plt.yticks(color='k', fontsize=14)\n",
        "        plt.grid(True)\n",
        "        plt.savefig(args.model+'beta-20'+time)\n"
      ]
    }
  ]
}
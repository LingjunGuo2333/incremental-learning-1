# -*- coding: utf-8 -*-
"""CIFAR-10-Resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/147UWhdtpPdEzlqaIc_iIQN06dYTTPsvV
"""

# Imports
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import random_split
from torch.utils.data import DataLoader
from torchvision.utils import make_grid
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from numpy import random
import matplotlib.pyplot as plt
import copy
from IPython.display import clear_output 

# fix random seed
clear_output()
seed = 0
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True
# plotting style
plt.style.use('seaborn-paper')
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['figure.dpi'] = 150

# Define Data Loaders

# load training data
def load_train_data(batch_size, param_mean, param_std, num_workers):
    transform_train = transforms.Compose([
        torchvision.transforms.RandomCrop(32, padding=4),
        torchvision.transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(param_mean, param_std),
    ])

    train_set = torchvision.datasets.CIFAR10(root='./data',
                                              train=True,
                                              download=True,
                                              transform=transform_train)

    train_loader = torch.utils.data.DataLoader(train_set,
                                               batch_size=batch_size,
                                               shuffle=True,
                                               num_workers=num_workers)

    return train_loader

# load test data and validation data (here, test == validation)
def load_test_data(batch_size, param_mean, param_std, num_workers):

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(param_mean, param_std),
    ])

    test_set = torchvision.datasets.CIFAR10(root='./data',
                                             train=False,
                                             download=True,
                                             transform=transform_test)
    test_loader = torch.utils.data.DataLoader(test_set,
                                              batch_size=batch_size,
                                              shuffle=False,
                                              num_workers=num_workers)

    return test_loader

# Specify device and load data to device.
def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

# define network
def create_model():
    model = torchvision.models.resnet18(pretrained=False, num_classes=10)
    model.conv1 = nn.Conv2d(3,
                            64,
                            kernel_size=(3, 3),
                            stride=(1, 1),
                            padding=(1, 1),
                            bias=False)
    model.maxpool = nn.Identity()
    model.fc = nn.Linear(in_features=512, out_features=10, bias=False)
    return model

# Define a hook
features = {}
def get_features(name):
    def hook(model, input, output):
        features[name] = output.detach()
    return hook

def initial_test(model, val_loader, device):
    # validation phase
    model.eval()
    f_out = torch.empty((0, 10))
    with torch.no_grad():
        correct, total = 0, 0
        num_batch = len(val_loader)
        for _, data in enumerate(val_loader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            f_out = torch.vstack((f_out, outputs.cpu()))
    return f_out

def model_train(model, device, train_loader, val_loader, EPOCH, feature_name, criterion, optimizer, scheduler):
    training_accs = []
    test_accs = []
    epoch_state_dict = {}
    best_acc = 0
    F_out = {}
    CheckPoint = {}
    for epoch in range(EPOCH):
        model.train()
        correct, sum_loss, total = 0, 0, 0
        for _, data in enumerate(train_loader, 0):
            # prepare data
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(F.softmax(outputs, 1) , F.one_hot(labels, num_classes=10).float())
            loss.backward()
            optimizer.step()

            sum_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        training_acc = correct / total
        print('[epoch:%d] Loss: %.05f | Acc: %.2f%%' %
              (epoch + 1, sum_loss, 100. * training_acc))
        training_accs.append(training_acc)

        # validation phase
        model.eval()
        FEATS = []
        f_out = torch.empty((0, 10))
        with torch.no_grad():
            correct, total = 0, 0
            num_batch = len(val_loader)
            for _, data in enumerate(val_loader, 0):
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                f_out = torch.vstack((f_out, outputs.cpu()))
                if epoch == EPOCH-1:
                    FEATS.append(features[feature_name].cpu().numpy())
            F_out[epoch+1] = f_out
            test_acc = correct / total
            test_accs.append(test_acc)
            print('[epoch:%d] Test Acc: %.2f%%' %
                  (epoch + 1, 100 * test_acc))
            if best_acc < test_acc:
                best_acc = test_acc
          # torch.save(model, "res18_" + str(epoch + 1))
        if epoch == EPOCH-1:
            phi = torch.flatten(torch.from_numpy(FEATS[0]), 1)
            for i in range(num_batch-1):
                phi = torch.vstack((phi, torch.flatten(torch.from_numpy(FEATS[i+1]), 1)))
        scheduler.step()
        # save model parameters
        checkpoint = {'epoch': epoch,
            'training_acc': training_acc,
            'loss': sum_loss, 
            'test_acc': test_acc,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict()}

        CheckPoint[epoch] = copy.deepcopy(checkpoint)

        # plot
        plt.plot(training_accs, linewidth=2, label='train acc')
        plt.plot(test_accs, linewidth=2, label='test acc')
        plt.legend(loc='best')
        plt.xlim(0, epoch + 1)
        plt.ylim(0, 1)
        plt.xlabel("epoch")
        plt.ylabel("accuracy")
        plt.legend(prop={'size': 18})
        axes = plt.gca()
        axes.xaxis.label.set_size(18)
        axes.yaxis.label.set_size(18)
        plt.xticks(color='k', fontsize=14)
        plt.yticks(color='k', fontsize=14)
        plt.grid(True)
        plt.tight_layout()
        plt.savefig('resnet_18')
        plt.clf()

    return model, best_acc, CheckPoint, phi, F_out

# TRAIN
if __name__ == "__main__":

    # Define hyperparameters
    EPOCH = 310
    batch_size = 32
    num_workers = 0
    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',
               'ship', 'truck')
    LR = 1e-1

    # load data and model to device
    param_mean = (0.4914, 0.4822, 0.4465)
    param_std = (0.2471, 0.2435, 0.2616)    # param_mean, param_std = get_mean_and_std(dataset_path)
    train_loader = load_train_data(batch_size, param_mean, param_std, num_workers)
    val_loader = load_test_data(batch_size, param_mean, param_std, num_workers)
    device = get_default_device()

    # Register a hook for the last layer
    model = create_model().to(device) 
    feature_name = 'phi'
    handle = model.avgpool.register_forward_hook(get_features(feature_name)) 
    # model = torch.load("CIFAR_BASELINE").to(device)
    # optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-6)
    # optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-8)
    optimizer = torch.optim.SGD(model.parameters(),
                                lr=LR,
                                momentum=0.9,
                                weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                step_size=50,
                                                gamma=0.33)
    criterion = nn.MSELoss()
    # Start Training model, train_dl, vali_dl, EPOCH, criterion, optimizer, scheduler


    init_f_out = initial_test(model, val_loader, device)
    model, best_acc, check_point, phi, F_out = model_train(model, device, train_loader, val_loader, EPOCH, feature_name, criterion, optimizer, scheduler)
    F_out[0] = init_f_out
    # save model
    handle.remove()
    W_infity = check_point[EPOCH-1]['model_state_dict']['fc.weight']
    torch.save(model, 'resnet18') 
    torch.save(check_point, 'check_point')
    torch.save(phi, 'phi')
    torch.save(F_out, 'F_out_ns')
    torch.save(W_infity, 'W_inf')
    # End Training
    print("Training finished! The best accuracy is ", best_acc)


    
